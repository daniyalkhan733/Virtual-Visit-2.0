<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>AI</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            background-color: #f0f0f0;
            padding: 20px;
        }

        pre {
            background-color: #1e1e1e;
            color: #dcdcdc;
            padding: 15px;
            border-radius: 5px;
            overflow-x: auto;
        }

        h1 {
            text-align: center;
        }
    </style>
</head>

<body>
    <h1>Exp 03 (To implement linear regression in python.)</h1>
    <pre><code class="language-python">
        import numpy as np
        import matplotlib.pyplot as plt
        
        # Sample data
        X = np.array([1, 2, 3, 4, 5])
        y = np.array([2, 4, 5, 4, 5])
        
        # Mean of X and y
        mean_x = np.mean(X)
        mean_y = np.mean(y)
        
        # Total number of values
        n = len(X)
        
        # Calculating the slope (m) and intercept (c)
        numerator = np.sum((X - mean_x) * (y - mean_y))
        denominator = np.sum((X - mean_x) ** 2)
        
        m = numerator / denominator
        c = mean_y - m * mean_x
        
        print(f"Linear Regression Line: y = {m:.2f}x + {c:.2f}")
        
        # Predicting values
        y_pred = m * X + c
        
        # Plotting the data
        plt.scatter(X, y, color='blue', label='Original Data')
        plt.plot(X, y_pred, color='red', label='Regression Line')
        plt.xlabel('X')
        plt.ylabel('y')
        plt.title('Linear Regression Example')
        plt.legend()
        plt.show()
        

    </code></pre>
    <h1>Exp 04 (To implement confusion matrix in python.)</h1>
    <pre><code class="language-python">
    import numpy as np
    from sklearn.metrics import confusion_matrix
        import seaborn as sns
        import matplotlib.pyplot as plt
        
        # Sample true and predicted values
        y_true = [1, 0, 1, 1, 0, 1, 0, 0, 1, 0]
        y_pred = [1, 0, 1, 0, 0, 1, 1, 0, 1, 0]
        
        # ------------------------------
        # 1. Manual Confusion Matrix
        # ------------------------------
        TP = TN = FP = FN = 0
        
        for actual, predicted in zip(y_true, y_pred):
            if actual == 1 and predicted == 1:
                TP += 1
            elif actual == 0 and predicted == 0:
                TN += 1
            elif actual == 0 and predicted == 1:
                FP += 1
            elif actual == 1 and predicted == 0:
                FN += 1
        
        print("Manual Confusion Matrix:")
        print(f"TP: {TP}, FP: {FP}")
        print(f"FN: {FN}, TN: {TN}\n")
        
        # ------------------------------
        # 2. Using scikit-learn
        # ------------------------------
        cm = confusion_matrix(y_true, y_pred)
        print("Sklearn Confusion Matrix:")
        print(cm)
        
        # ------------------------------
        # 3. Heatmap Visualization
        # ------------------------------
        plt.figure(figsize=(6, 4))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Pred 0', 'Pred 1'], yticklabels=['Actual 0', 'Actual 1'])
        plt.xlabel('Predicted')
        plt.ylabel('Actual')
        plt.title('Confusion Matrix Heatmap')
        plt.tight_layout()
        plt.show()
        
        </code></pre>
    <h1>Exp 05 (To implement logistic regression in python)</h1>
    <pre><code class="language-python">
        from sklearn.linear_model import LogisticRegression
        from sklearn.metrics import confusion_matrix, accuracy_score
        
        # Sample data
        X = np.array([[20], [25], [30], [35], [40]])
        y = np.array([0, 0, 1, 1, 1])
        
        # Train logistic regression model
        model = LogisticRegression()
        model.fit(X, y)
        
        # Predict
        predictions = model.predict(X)
        
        print("Predictions:", predictions)
        print("Accuracy:", accuracy_score(y, predictions))
        print("Confusion Matrix:\n", confusion_matrix(y, predictions))
        
    
            </code></pre>
    <h1>Exp 06 (To implement SVM in python.)</h1>
    <pre><code class="language-python">
        from sklearn import datasets
        from sklearn.model_selection import train_test_split
        from sklearn.svm import SVC
        from sklearn.metrics import classification_report, confusion_matrix
        import matplotlib.pyplot as plt
        import seaborn as sns
        
        # Load sample dataset (Iris)
        iris = datasets.load_iris()
        X = iris.data
        y = iris.target
        
        # Only use 2 classes for binary classification (optional)
        X = X[y != 2]
        y = y[y != 2]
        
        # Split data
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
        
        # Train SVM model
        model = SVC(kernel='linear')  # Try 'rbf', 'poly' etc.
        model.fit(X_train, y_train)
        
        # Predict
        y_pred = model.predict(X_test)
        
        # Results
        print("Confusion Matrix:")
        cm = confusion_matrix(y_test, y_pred)
        print(cm)
        
        print("\nClassification Report:")
        print(classification_report(y_test, y_pred))
        
        # Heatmap
        plt.figure(figsize=(6,4))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', xticklabels=['Class 0', 'Class 1'], yticklabels=['Class 0', 'Class 1'])
        plt.xlabel('Predicted')
        plt.ylabel('Actual')
        plt.title('SVM Confusion Matrix')
        plt.tight_layout()
        plt.show()
                
                </code></pre>
    <h1>Exp 07 (To implement Hebbian learning rule in python. )</h1>
    <pre><code class="language-python">
        import numpy as np

        # Input vectors (samples) and desired outputs
        X = np.array([
            [1, 1],
            [1, -1],
            [-1, 1],
            [-1, -1]
        ])
        
        # Target output for each input
        # Let's say we want the neuron to learn an AND-like pattern
        y = np.array([1, -1, -1, -1])
        
        # Initialize weights and learning rate
        weights = np.zeros(X.shape[1])
        learning_rate = 0.1
        
        # Hebbian learning
        for i in range(len(X)):
            x_i = X[i]
            y_i = y[i]
            delta_w = learning_rate * x_i * y_i
            weights += delta_w
            print(f"After sample {i+1}, updated weights: {weights}")
        
        print("\nFinal learned weights:", weights)
        
        # Predicting using sign activation
        def predict(input_vector, weights):
            result = np.dot(input_vector, weights)
            return 1 if result >= 0 else -1
        
        # Test predictions
        print("\nPredictions:")
        for x in X:
            output = predict(x, weights)
            print(f"Input: {x}, Predicted Output: {output}")
        
                </code></pre>
    <h1>Exp 08 (To implement Logic Gate using Mc-Culloch Pitts model.)</h1>
    <pre><code class="language-python">
        def mcp_neuron(inputs, weights, threshold):
        summation = sum(i * w for i, w in zip(inputs, weights))
        return 1 if summation >= threshold else 0
    
    # Logic Gates
    def AND_gate():
        print("AND Gate")
        weights = [1, 1]
        threshold = 2
        for x in [(0, 0), (0, 1), (1, 0), (1, 1)]:
            output = mcp_neuron(x, weights, threshold)
            print(f"Input: {x}, Output: {output}")
    
    def OR_gate():
        print("\nOR Gate")
        weights = [1, 1]
        threshold = 1
        for x in [(0, 0), (0, 1), (1, 0), (1, 1)]:
            output = mcp_neuron(x, weights, threshold)
            print(f"Input: {x}, Output: {output}")
    
    def NOT_gate():
        print("\nNOT Gate")
        weights = [-1]
        threshold = 0
        for x in [0, 1]:
            output = mcp_neuron([x], weights, threshold)
            print(f"Input: {x}, Output: {output}")
    
    # Run gates
    AND_gate()
    OR_gate()
    NOT_gate()
    
                </code></pre>
    <h1>
        Exp 09 (To implement Error Backpropagation Perceptron Training Algorithm)
    </h1>
    <pre><code class="language-python">
        import numpy as np

        # Activation function: Step function
        def step_function(x):
            return 1 if x >= 0 else 0
        
        # Perceptron Training Algorithm
        def train_perceptron(X, y, learning_rate=0.1, epochs=10):
            n_samples, n_features = X.shape
            weights = np.zeros(n_features)
            bias = 0
        
            for epoch in range(epochs):
                print(f"\nEpoch {epoch+1}")
                for i in range(n_samples):
                    linear_output = np.dot(X[i], weights) + bias
                    y_pred = step_function(linear_output)
                    error = y[i] - y_pred
        
                    # Update rule (backpropagation of error)
                    weights += learning_rate * error * X[i]
                    bias += learning_rate * error
        
                    print(f"Input: {X[i]}, Target: {y[i]}, Predicted: {y_pred}, Updated Weights: {weights}, Bias: {bias}")
            
            return weights, bias
        
        # Predict function
        def predict(X, weights, bias):
            linear_output = np.dot(X, weights) + bias
            return [step_function(x) for x in linear_output]
        
        # Dataset (AND gate)
        X = np.array([
            [0, 0],
            [0, 1],
            [1, 0],
            [1, 1]
        ])
        y = np.array([0, 0, 0, 1])  # AND output
        
        # Train
        weights, bias = train_perceptron(X, y)
        
        # Test
        print("\nFinal Predictions:")
        outputs = predict(X, weights, bias)
        for i, x in enumerate(X):
            print(f"Input: {x}, Output: {outputs[i]}")
        </code></pre>
    <h1>Exp 10 ( To implement Principal Component Analysis)</h1>
    <pre><code class="language-python">
        import numpy as np
        import matplotlib.pyplot as plt
        
        # Sample 2D data
        X = np.array([
            [2.5, 2.4],
            [0.5, 0.7],
            [2.2, 2.9],
            [1.9, 2.2],
            [3.1, 3.0],
            [2.3, 2.7],
            [2, 1.6],
            [1, 1.1],
            [1.5, 1.6],
            [1.1, 0.9]
        ])
        
        # Step 1: Standardize data (mean = 0)
        X_meaned = X - np.mean(X, axis=0)
        
        # Step 2: Covariance matrix
        cov_matrix = np.cov(X_meaned, rowvar=False)
        
        # Step 3: Eigen decomposition
        eigen_values, eigen_vectors = np.linalg.eigh(cov_matrix)
        
        # Step 4: Sort eigenvalues & eigenvectors
        sorted_index = np.argsort(eigen_values)[::-1]
        eigen_values = eigen_values[sorted_index]
        eigen_vectors = eigen_vectors[:, sorted_index]
        
        # Step 5: Project data to top k components
        k = 2
        eigenvector_subset = eigen_vectors[:, :k]
        X_reduced = np.dot(X_meaned, eigenvector_subset)
        
        print("Reduced Data:\n", X_reduced)
        
        # Plot the reduced 2D data
        plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c='green')
        plt.title("PCA - Projected Data")
        plt.xlabel("Principal Component 1")
        plt.ylabel("Principal Component 2")
        plt.grid(True)
        plt.show()
            </code></pre>
</body>

</html>