<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>BC</title>
    <style>
      body {
        font-family: Arial, sans-serif;
        background-color: #f0f0f0;
        padding: 20px;
      }

      pre {
        background-color: #1e1e1e;
        color: #dcdcdc;
        padding: 15px;
        border-radius: 5px;
        overflow-x: auto;
      }

      h1 {
        text-align: center;
      }
    </style>
  </head>

  <body>
       <h1>Exp 1: To Explore Python libraries for deep
learning example Tensor flow, Keras etc.</h1>
<pre><code class="language-python">
# importing the dependencies
import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()
import numpy as np
import matplotlib.pyplot as plt
# Model Parameters
learning_rate = 0.01 # determines the step size at each iteration while moving toward a minimum of a loss function.
training_epochs = 2000 # An epoch is when all the training data is used at once and is defined as the total number of iteration
display_step = 200
# Training Data
train_X = np.asarray([3.3,4.4,5.5,6.71,6.93,4.168,9.779,6.182,7.59,2.167,7.042,10.791,5.313,7.997,5.654,9.27,3.1])
train_y = np.asarray([1.7,2.76,2.09,3.19,1.694,1.573,3.366,2.596,2.53,1.221,2.827,3.465,1.65,2.904,2.42,2.94,1.3])
n_samples = train_X.shape[0] # assigns the number of samples in the train_X dataset to the variable n_samples.
# Test Data
test_X = np.asarray([6.83, 4.668, 8.9, 7.91, 5.7, 8.7, 3.1, 2.1]) # in NumPy it is used to convert a given input (such as a list, tuple, or array-like object) into an array.
test_y = np.asarray([1.84, 2.273, 3.2, 2.831, 2.92, 3.24, 1.35, 1.03])
# Set placeholders for feature and target vectors
X = tf.placeholder(tf.float32) # sed to define a placeholder tensor that could be later fed with actual data during the execution of a computational graph.
y = tf.placeholder(tf.float32) # float32, indicates that it will hold floating-point values.
# Set model weights and bias
W = tf.Variable(np.random.randn(), name="weight")
b = tf.Variable(np.random.randn(), name="bias")
# Construct a linear model
linear_model = W*X + b
# Mean squared error
cost = tf.reduce_sum(tf.square(linear_model - y)) / (2*n_samples) # represents the mean squared error (MSE) cost function for a linear regression model.
# Gradient descent
optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost) # TensorFlow sets up the computation to minimize the cost function using gradient descent.
# Initializing the variables
init = tf.global_variables_initializer() # get an operation that, when executed, initializes all the global variables in the TensorFlow graph
# Launch the graph
with tf.Session() as sess: # manager that creates a TensorFlow session and automatically manages its execution.
# Load initialized variables in current session
 sess.run(init)
 # Fit all training data
 for epoch in range(training_epochs):
# perform gradient descent step
  sess.run(optimizer, feed_dict={X: train_X, y: train_y})
  # Display logs per epoch step
 if (epoch+1) % display_step == 0:
  c = sess.run(cost, feed_dict={X: train_X, y: train_y})
 print("Epoch:{0:6} \t Cost:{1:10.4} \t W:{2:6.4} \t b:{3:6.4}". # specify the values you want to print in place of the placeholders {0}, {1}, {2}, and {3}.
 format(epoch+1, c, sess.run(W), sess.run(b)))
 # Print final parameter values
 print("Optimization Finished!")
 training_cost = sess.run(cost, feed_dict={X: train_X, y: train_y})
 print("Final training cost:", training_cost, "W:", sess.run(W), "b:",
  sess.run(b), '\n')
 # Graphic display
 plt.plot(train_X, train_y, 'ro', label='Original data')
 plt.plot(train_X, sess.run(W) * train_X + sess.run(b), label='Fitted line')
 plt.legend()
 plt.show()
 # Testing the model
 testing_cost = sess.run(tf.reduce_sum(tf.square(linear_model - y)) / (2 * test_X.shape[0]), feed_dict={X: test_X, y: test_y})
 print("Final testing cost:", testing_cost)
 print("Absolute mean square loss difference:", abs(training_cost - testing_cost))
# Display fitted line on test data
 plt.plot(test_X, test_y, 'bo', label='Testing data')
 plt.plot(train_X, sess.run(W) * train_X + sess.run(b), label='Fitted line')
 plt.legend()
 plt.show()
 # PyTorch
# -*- coding: utf-8 -*-
import torch # developing and training neural network based deep learning models.
import math # extends the list of mathematical functions.
dtype = torch.float
device = torch.device("cpu")
# device = torch.device("cuda:0") # Uncomment this to run on GPU
# Create random input and output data
x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)
y = torch.sin(x)
# Randomly initialize weights
a = torch.randn((), device=device, dtype=dtype)
b = torch.randn((), device=device, dtype=dtype)
c = torch.randn((), device=device, dtype=dtype)
d = torch.randn((), device=device, dtype=dtype)
learning_rate = 1e-6
for t in range(2000):
# Forward pass: compute predicted y
 y_pred = a + b * x + c * x ** 2 + d * x ** 3
 # Compute and print loss
loss = (y_pred - y).pow(2).sum().item()
if t % 100 == 99:
 print(t, loss)
# Backprop to compute gradients of a, b, c, d with respect to loss
grad_y_pred = 2.0 * (y_pred - y)
grad_a = grad_y_pred.sum()
grad_b = (grad_y_pred * x).sum()
grad_c = (grad_y_pred * x ** 2).sum() # calculates the gradient of some scalar variable c with respect to the tensor x in PyTorch.
grad_d = (grad_y_pred * x ** 3).sum()
# Update weights using gradient descent
a -= learning_rate * grad_a # updates the value of some scalar variable a using gradient descent in PyTorch.
b -= learning_rate * grad_b
c -= learning_rate * grad_c
d -= learning_rate * grad_d
print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')
</code></pre>
   <h1>Exp 2: Implement M'c Culloch Pitts model
for binary logic function.</h1>
<pre><code class="language-python">
x1 = [0, 0, 1, 1]  #x1, x2 represent the binary input values.
x2 = [0, 1, 0, 1]
w1 = [1, 1, 1, 1]  #w1 and w2 represent the weights corresponding to each input value.
w2 = [1, 1, 1, 1]
t = 2  #The variable t is set to 2, which represents the threshold for the AND gate.
#output
print("x1 x2 w1 w2 t O")  #prints a header line "x1 x2 w1 w2 t O" and then iterates over each index i of the x1
for i in range(len(x1)):
 if (x1[i] * w1[i] + x2[i] * w2[i]) >= t:
        print(x1[i], ' ', x2[i], ' ', w1[i], ' ', w2[i], ' ', t, ' ', 1) #If the sum is greater than or equal to the threshold t, it prints the input values, weights, threshold, and 1
else:
        print(x1[i], ' ', x2[i], ' ', w1[i], ' ', w2[i], ' ', t, ' ', 0) #or 0

import numpy as np

input_table = np.array([  #2D array input_table is defined using the np.array function
    [0, 0],
    [0, 1],
    [1, 0],
    [1, 1]
])

print(f'input table:\n{input_table}')

import numpy as np
def linear_threshold_gate(dot_product, threshold):
    if dot_product >= threshold:
        return 1 #linear_threshold_gate function is defined, which takes a dot product and a threshold as input and returns 1
    else:
        return 0 #if the dot product is greater than or equal to the threshold, and 0 otherwise.

input_table = np.array([
    [0, 0],
    [0, 1],
    [1, 0],
    [1, 1]
])

weights = np.array([1, 1])
print(f'weights: {weights}')

# Dot product matrix of inputs and weights
dot_products = input_table @ weights #dot product between input_table and weights is calculated using the @ operator,
print(f'Dot products: {dot_products}')

T = 2
for i in range(0, 4):
    activation = linear_threshold_gate(dot_products[i], T) #The linear_threshold_gate function is called for each dot product and threshold combination
    print(f'Activation: {activation}')
</code></pre>
    <h1>Exp 3: Implement a backpropagation
algorithm to train a DNN with atleast two
hidden layers</h1>
    <pre><code class="language-python">
import numpy as np
import matplotlib.pyplot as plt

class ModernDeepNN:

    def __init__(self, input_size, hidden1, hidden2, output_size, lr=0.1):
        # He Weight Initialization (optimal for ReLU)
        self.W1 = np.random.randn(input_size, hidden1) * np.sqrt(2. / input_size)
        self.b1 = np.zeros((1, hidden1))
        self.W2 = np.random.randn(hidden1, hidden2) * np.sqrt(2. / hidden1)
        self.b2 = np.zeros((1, hidden2))
        self.W3 = np.random.randn(hidden2, output_size) * np.sqrt(2. / hidden2)
        self.b3 = np.zeros((1, output_size))

        self.lr = lr
        self.loss_history = []

    def _relu(self, x):
        return np.maximum(0, x)

    def _relu_derivative(self, x):
        # Derivative is 1 for x > 0, and 0 otherwise
        return (x > 0) * 1

    def _softmax(self, x):
        # Subtract max for numerical stability (prevents overflow)
        exp_scores = np.exp(x - np.max(x, axis=1, keepdims=True))
        return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)

    def _compute_loss(self, y, output_probs):
        """
        Categorical Cross-Entropy Loss
        """
        m = y.shape[0]
        # Add a small epsilon for numerical stability (prevents log(0))
        epsilon = 1e-9
        loss = -np.sum(y * np.log(output_probs + epsilon)) / m
        return loss

    def forward(self, X):
        # Layer 1 (ReLU)
        self.z1 = np.dot(X, self.W1) + self.b1
        self.a1 = self._relu(self.z1)

        # Layer 2 (ReLU)
        self.z2 = np.dot(self.a1, self.W2) + self.b2
        self.a2 = self._relu(self.z2)

        # Output Layer (Softmax)
        self.z3 = np.dot(self.a2, self.W3) + self.b3
        self.a3 = self._softmax(self.z3)
        return self.a3

    def backward(self, X, y, output):
        m = X.shape[0]

        # Output layer gradient (Cross-Entropy with Softmax derivative)
        dZ3 = output - y
        dW3 = np.dot(self.a2.T, dZ3) / m
        db3 = np.sum(dZ3, axis=0, keepdims=True) / m

        # Hidden layer 2 gradient
        dA2 = np.dot(dZ3, self.W3.T)
        dZ2 = dA2 * self._relu_derivative(self.z2)
        dW2 = np.dot(self.a1.T, dZ2) / m
        db2 = np.sum(dZ2, axis=0, keepdims=True) / m

        # Hidden layer 1 gradient
        dA1 = np.dot(dZ2, self.W2.T)
        dZ1 = dA1 * self._relu_derivative(self.z1)
        dW1 = np.dot(X.T, dZ1) / m
        db1 = np.sum(dZ1, axis=0, keepdims=True) / m

        # Update weights and biases
        self.W3 -= self.lr * dW3
        self.b3 -= self.lr * db3
        self.W2 -= self.lr * dW2
        self.b2 -= self.lr * db2
        self.W1 -= self.lr * dW1
        self.b1 -= self.lr * db1

    def train(self, X, y, epochs, print_every=100):
        for epoch in range(epochs):
            output = self.forward(X)
            self.backward(X, y, output)

            loss = self._compute_loss(y, output)
            self.loss_history.append(loss)

            if epoch % print_every == 0 or epoch == epochs - 1:
                accuracy = self.get_accuracy(X, y)
                print(f"Epoch: {epoch:4d} | Loss: {loss:.6f} | Accuracy: {accuracy*100:.2f}%")

    def predict(self, X):
        # Perform a forward pass and return the index of the highest probability
        probabilities = self.forward(X)
        return np.argmax(probabilities, axis=1)

    def get_accuracy(self, X, y):
        pred_classes = self.predict(X)
        true_classes = np.argmax(y, axis=1)
        return np.mean(pred_classes == true_classes)


# 1. Input data (features)
X = np.array([
    [0.1, 0.2, 0.3], [0.4, 0.5, 0.6],
    [0.7, 0.8, 0.9], [0.2, 0.3, 0.1],
    [0.8, 0.1, 0.7], [0.5, 0.9, 0.2],
])

# 2. Actual output (one-hot encoded targets)
y = np.array([
    [1, 0], [1, 0], [0, 1],
    [1, 0], [0, 1], [1, 0],
])

# 3. Create and train the network
print("--- Training Network ---")
nn = ModernDeepNN(input_size=3, hidden1=4, hidden2=3, output_size=2, lr=0.1)
nn.train(X, y, epochs=10, print_every=2)

# 4. Final results
final_accuracy = nn.get_accuracy(X, y)
final_predictions_probs = nn.forward(X)

print("\n--- Final Results ---")
print("1. INPUT VALUES:")
print(X)
print("\n2. ACTUAL OUTPUT (One-Hot):")
print(y)
print("\n3. PREDICTED PROBABILITIES:")
print(np.round(final_predictions_probs, 4))
print("\n4. FINAL ACCURACY:")
print(f"Accuracy: {final_accuracy:.4f} ({final_accuracy*100:.2f}%)")



    </code></pre>
    <h1>Exp 4: Apply any one of stochastic gradient
descent learning algorithms to learn the
parameters of the supervised single layer feed
forward neural network.</h1>
    <pre><code class="language-python">
 import pandas as pd
import numpy as np
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
import matplotlib.pyplot as plt
import seaborn as sns


data = {'price': np.random.randint(100000, 500000, 500),
            'area': np.random.randint(1500, 5000, 500),
            'bedrooms': np.random.randint(1, 5, 500),
            'bathrooms': np.random.randint(1, 4, 500),
            'furnishingstatus': ['furnished', 'semi-furnished', 'unfurnished'] * 166 + ['furnished', 'semi-furnished']}
df = pd.DataFrame(data)

categorical_cols = df.select_dtypes(include=['object']).columns
df = pd.get_dummies(df, columns=categorical_cols, drop_first=True, dtype=int)

X = df.drop('price', axis=1)
y = df['price']

X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.3, random_state=42)

feature_scaler = MinMaxScaler()
X_train_scaled = feature_scaler.fit_transform(X_train)
X_valid_scaled = feature_scaler.transform(X_valid)

target_scaler = MinMaxScaler()
y_train_scaled = target_scaler.fit_transform(y_train.values.reshape(-1, 1))
y_valid_scaled = target_scaler.transform(y_valid.values.reshape(-1, 1))

activations = ['relu', 'tanh', 'sigmoid', 'linear']
histories = {}
best_model = None
best_val_loss = float('inf')
best_activation = ""

print("\nStarting model training to compare different activation functions...")

for activation_func in activations:
    print(f"\n--- Training with '{activation_func}' activation ---")

    # Define the model architecture
    model = tf.keras.Sequential([
        tf.keras.layers.Dense(64, activation=activation_func, input_shape=[X_train_scaled.shape[1]]),
        tf.keras.layers.Dense(32, activation=activation_func),
        tf.keras.layers.Dense(1, activation='linear')
    ])

    # Compile the model
    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_absolute_error'])

    # Print model summary for the first model to show architecture
    if activation_func == activations[0]:
        print("\nModel Architecture:")
        model.summary()

    # Train the model
    history = model.fit(
        X_train_scaled, y_train_scaled,
        validation_data=(X_valid_scaled, y_valid_scaled),
        epochs=10,
        verbose=0
    )

    # Store history for plotting
    histories[activation_func] = history
    final_loss, final_mae = model.evaluate(X_valid_scaled, y_valid_scaled, verbose=0)

    print(f"Training Complete!")
    print(f"Final Validation Loss (MSE): {final_loss:.4f}")
    print(f"Final Validation MAE: {final_mae:.4f}")

    if final_loss < best_val_loss:
        best_val_loss = final_loss
        best_model = model
        best_activation = activation_func

print(f"\nBest performing activation function: '{best_activation}' with a validation loss of {best_val_loss:.4f}")

# --- 7. Visualize Training History ---
sns.set_style("whitegrid")
plt.figure(figsize=(16, 7))

# Plot Validation Loss (MSE)
plt.subplot(1, 2, 1)
for activation_func, history in histories.items():
    plt.plot(history.history['val_loss'], label=f'{activation_func.capitalize()}')
plt.title('Model Comparison: Validation Loss (MSE)', fontsize=16)
plt.xlabel('Epochs', fontsize=12)
plt.ylabel('Mean Squared Error', fontsize=12)
plt.legend()
plt.ylim(bottom=0)

# Plot Validation Mean Absolute Error (MAE)
plt.subplot(1, 2, 2)
for activation_func, history in histories.items():
    plt.plot(history.history['val_mean_absolute_error'], label=f'{activation_func.capitalize()}')
plt.title('Model Comparison: Validation MAE', fontsize=16)
plt.xlabel('Epochs', fontsize=12)
plt.ylabel('Mean Absolute Error', fontsize=12)
plt.legend()
plt.ylim(bottom=0)

plt.tight_layout()
plt.show()


# Get a few samples from the validation set
num_samples = 5
X_sample = X_valid_scaled[:num_samples]
y_actual_sample = y_valid.iloc[:num_samples].values

# Predict and inverse transform to get the actual price scale
predictions_scaled = best_model.predict(X_sample)
predictions = target_scaler.inverse_transform(predictions_scaled).flatten()

# Display the results
print("\n--- Sample Predictions vs. Actual Prices ---")
prediction_df = pd.DataFrame({
    'Actual Price': y_actual_sample,
    'Predicted Price': predictions
})
prediction_df['Difference'] = prediction_df['Actual Price'] - prediction_df['Predicted Price']

# Format for better readability
pd.options.display.float_format = '${:,.2f}'.format
print(prediction_df)


    </code></pre>
    <h1>Exp 5: Design the architecture and
implement the autoencoder model for image
compression</h1>
    <pre><code class="language-python">
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, models, optimizers, datasets
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns


# 1. Load and Prepare the Dataset (e.g., MNIST)
(x_train, _), (x_test, _) = datasets.mnist.load_data()

# Normalize and reshape the images
x_train = x_train.astype('float32') / 255.0
x_test = x_test.astype('float32') / 255.0

x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))
x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))

print(f"Original image shape: {x_train.shape[1]}")
print(f"x_train shape: {x_train.shape}")
print(f"x_test shape: {x_test.shape}")

# Plot a few original images from the dataset
plt.figure(figsize=(10, 4))
for i in range(5):
    ax = plt.subplot(1, 5, i + 1)
    plt.imshow(x_train[i].reshape(28, 28))
    plt.gray()
    ax.get_xaxis().set_visible(False)
    ax.get_yaxis().set_visible(False)
plt.suptitle('Original MNIST Images')
plt.show()

# 2. Define the Encoder Model
latent_dim = 32
encoder_input = keras.Input(shape=(784,), name="encoder_input")
encoder_dense_layer1 = layers.Dense(300, name="encoder_dense_layer1")(encoder_input)
encoder_activ_layer1 = layers.LeakyReLU(name="encoder_leaky_relu1")(encoder_dense_layer1)
encoder_output = layers.Dense(latent_dim, activation='relu', name="encoder_output")(encoder_activ_layer1)
encoder = models.Model(inputs=encoder_input, outputs=encoder_output, name="encoder")
encoder.summary()

# 3. Define the Decoder Model
decoder_input = keras.Input(shape=(latent_dim,), name="decoder_input")
decoder_dense_layer1 = layers.Dense(300, name="decoder_dense_layer1")(decoder_input)
decoder_activ_layer1 = layers.LeakyReLU(name="decoder_leaky_relu1")(decoder_dense_layer1)
decoder_output = layers.Dense(784, activation='sigmoid', name="decoder_output")(decoder_activ_layer1)
decoder = models.Model(inputs=decoder_input, outputs=decoder_output, name="decoder")
decoder.summary()

# 4. Combine Encoder and Decoder to form the Autoencoder
autoencoder_input = keras.Input(shape=(784,), name="autoencoder_input")
encoded_representation = encoder(autoencoder_input)
reconstructed_output = decoder(encoded_representation)
autoencoder = models.Model(inputs=autoencoder_input, outputs=reconstructed_output, name="autoencoder")
autoencoder.summary()

# 5. Compile and Train the Autoencoder
autoencoder.compile(optimizer='adam', loss='binary_crossentropy')

print("\nTraining the Autoencoder...")
history = autoencoder.fit(x_train, x_train,
                          epochs=10,
                          batch_size=256,
                          shuffle=True,
                          validation_data=(x_test, x_test),
                          verbose=0) # Set verbose to 0 to make output cleaner

print("Training finished.")

# Plot training history
plt.figure(figsize=(10, 5))
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Autoencoder Training Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

# 6. Evaluate and Get Reconstructions
encoded_imgs = encoder.predict(x_test)
decoded_imgs = decoder.predict(encoded_imgs)

# 7. Plot Original vs. Reconstructed Images
n = 10
plt.figure(figsize=(20, 4))
for i in range(n):
    # Original
    ax = plt.subplot(2, n, i + 1)
    plt.imshow(x_test[i].reshape(28, 28))
    plt.gray()
    ax.get_xaxis().set_visible(False)
    ax.get_yaxis().set_visible(False)
    if i == 0:
        ax.set_title("Original")

    # Reconstruction
    ax = plt.subplot(2, n, i + 1 + n)
    plt.imshow(decoded_imgs[i].reshape(28, 28))
    plt.gray()
    ax.get_xaxis().set_visible(False)
    ax.get_yaxis().set_visible(False)
    if i == 0:
        ax.set_title("Reconstructed")
plt.suptitle('Original vs. Reconstructed Images')
plt.show()

# 8. NEW: Visualize the Encoded Representation (Heatmap)
# This shows the compressed, 32-dimensional vector for a few test images.
plt.figure(figsize=(20, 8))
for i in range(5):
    # Original Image
    ax = plt.subplot(2, 5, i + 1)
    plt.imshow(x_test[i].reshape(28, 28), cmap='gray')
    ax.set_title("Original Image")
    ax.axis('off')

    # Encoded Heatmap
    ax = plt.subplot(2, 5, i + 6)
    # Reshape the encoded vector to 8x4 for better visualization
    sns.heatmap(encoded_imgs[i].reshape(8, 4), cmap='viridis')
    ax.set_title("Encoded Heatmap")
    ax.axis('off')

plt.suptitle('Encoded Representation Heatmaps', fontsize=16)
plt.show()


# 9. NEW: Original vs. Reconstructed Pixel Comparison Graph
# Let's pick one image to compare
image_index = 0
original_pixels = x_test[image_index]
reconstructed_pixels = decoded_imgs[image_index]

plt.figure(figsize=(12, 6))
plt.plot(original_pixels, label='Original Pixels', color='blue', alpha=0.7)
plt.plot(reconstructed_pixels, label='Reconstructed Pixels', color='red', alpha=0.7, linestyle='--')
plt.title(f'Pixel-by-Pixel Comparison for Image #{image_index}')
plt.xlabel('Pixel Index (Flattened)')
plt.ylabel('Pixel Intensity')
plt.legend()
plt.grid(True, linestyle='--', alpha=0.6)
plt.show()
            </code></pre>
     <h1>Exp 6: Design and implement a CNN model
for digit recognition.</h1>
    <pre><code class="language-python">
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers,models
import numpy as np
(X_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()
X_train.shape
X_train = X_train.reshape(-1,28,28,1)
x_test = x_test.reshape(-1,28,28,1)
X_train = X_train/255
x_test = x_test/255
covulational_neural_network = models.Sequential([
    layers.Conv2D(filters=32, kernel_size=(3,3), activation='relu', input_shape=(28,28,1)),
    layers.MaxPooling2D((2,2)),
    layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu'),
    layers.MaxPooling2D((2,2)),
    layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu'),
    layers.MaxPooling2D((2,2)),
    layers.Flatten(),
    layers.Dense(64, activation='relu'),
    layers.Dense(10, activation='softmax')
  ])
covulational_neural_network.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
covulational_neural_network.fit(X_train, y_train, epochs=10)
covulational_neural_network.evaluate(x_test, y_test)
y_predicted_by_model = covulational_neural_network.predict(x_test)
y_predicted_by_model[0]
np.argmax(y_predicted_by_model[0])
y_predicted_labels = [np.argmax(i)for i in y_predicted_by_model]
y_predicted_labels[:5]
        </code></pre>
 <h1>Exp 7: Design and implement a CNN model
for image compression.</h1> 
<h3>Dataset downlaod link below</h3>
<a href="https://filebin.net/97jonqvwybmgcreu" target="_blank">Dataset Link</a>
    <pre><code class="language-python">
!pip install keras

# Importing all necessary libraries
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D
from keras.layers import Activation, Dropout, Flatten, Dense
from keras import backend as K

img_width, img_height = 224, 224

train_data_dir = '/content/drive/MyDrive/Colab Notebooks/DL/train'
validation_data_dir = '/content/drive/MyDrive/Colab Notebooks/DL/test'
nb_train_samples = 400
nb_validation_samples = 100
epochs = 10
batch_size = 16

if K.image_data_format() == 'channels_first':
    input_shape = (3, img_width, img_height)
else:
    input_shape = (img_width, img_height, 3)

model = Sequential()
model.add(Conv2D(32, (2, 2), input_shape=input_shape))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(32, (2, 2)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(64, (2, 2)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Flatten())
model.add(Dense(64))
model.add(Activation('relu'))
model.add(Dropout(0.5))
model.add(Dense(1))
model.add(Activation('sigmoid'))

model.compile(loss='binary_crossentropy',
              optimizer='rmsprop',
              metrics=['accuracy'])


train_datagen = ImageDataGenerator(
    rescale=1. / 255,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True)

test_datagen = ImageDataGenerator(rescale=1. / 255)

train_generator = train_datagen.flow_from_directory(
    train_data_dir,
    target_size=(img_width, img_height),
    batch_size=batch_size,
    class_mode='binary')

validation_generator = test_datagen.flow_from_directory(
    validation_data_dir,
    target_size=(img_width, img_height),
    batch_size=batch_size,
    class_mode='binary')

model.fit(
    train_generator,
    steps_per_epoch=nb_train_samples // batch_size,
    epochs=epochs,
    validation_data=validation_generator,
    validation_steps=nb_validation_samples // batch_size)
model.save('/content/drive/MyDrive/Colab Notebooks/DL/model_saved.h5')

from keras.models import load_model
from keras.preprocessing.image import load_img
from keras.preprocessing.image import img_to_array
from keras.applications.vgg16 import preprocess_input
from keras.applications.vgg16 import decode_predictions
from keras.applications.vgg16 import VGG16
import numpy as np

from keras.models import load_model

model1 = load_model("/content/drive/MyDrive/Colab Notebooks/DL/model_saved.h5")

image = load_img('/content/drive/MyDrive/Colab Notebooks/DL/test/planes/5.jpg', target_size=(224, 224))
img = np.array(image)
img = img / 255.0
img = img.reshape(1,224,224,3)
label = model1.predict(img)
print("Predicted Class (0 - Cars , 1- Planes): ", label[0][0])


        </code></pre>
  </body>
</html>
